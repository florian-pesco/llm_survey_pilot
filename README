LLM Survey Response Time Prediction

This repository contains a simulation script for predicting median response times of survey questions using a Large Language Model (LLM). The goal of the project is to compare different prompt engineering strategies and assess their impact on predicted response times.

Project Structure

run_simulation.py
data/questions.json
prompts/baseline.txt
prompts/example.txt
prompts/chain_of_thought.txt
results/

run_simulation.py
Main execution script. Loads questions and prompt templates, calls the LLM, parses model output, and stores predicted response times.

data/questions.json
JSON file containing survey questions. Each entry includes a question ID, full question text, and a code domain describing the answer options.

prompts/
Contains prompt templates used for different prompt engineering strategies:

baseline.txt: Direct estimation without examples

example.txt: Estimation guided by example questions with known response times

chain_of_thought.txt: Step-by-step decomposition into reading, comprehension, and selection time

results/
Output directory. The script writes a CSV file containing predicted response times for each question and prompt technique.


LLM Backend and Extensibility

The simulation script is intentionally designed to keep the LLM interaction simple and transparent. All model-specific logic is contained in a single function:

call_llm(prompt)

This function acts as the only interface between the simulation logic and the underlying LLM or API.

To use a different LLM or API, users can modify or replace the implementation inside call_llm(prompt) without changing any other part of the simulation code. The rest of the pipeline (prompt construction, iteration logic, result storage, and evaluation) remains unchanged.
Any alternative LLM integration should follow the same contract:
    Input: a prompt string
    Output: the raw textual response from the model